{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM0Fz3L7VUsKPcqBLe9P6uS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"hQz6KObTPPF-","executionInfo":{"status":"ok","timestamp":1706683210816,"user_tz":-330,"elapsed":1166,"user":{"displayName":"Akshit Sharma","userId":"06858188963884428867"}}},"outputs":[],"source":["import numpy as np\n","import nltk\n","import string\n","import random"]},{"cell_type":"markdown","source":["# **Reading the text**"],"metadata":{"id":"-vGwgD6NSKnB"}},{"cell_type":"code","source":["f = open('/content/data.txt','r',errors = 'ignore')\n","raw_doc = f.read()"],"metadata":{"id":"QD0JqCFaPyhq","executionInfo":{"status":"ok","timestamp":1706683534629,"user_tz":-330,"elapsed":3,"user":{"displayName":"Akshit Sharma","userId":"06858188963884428867"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Text preprosing and tokenization"],"metadata":{"id":"kYWShmI7SRJt"}},{"cell_type":"code","source":["raw_doc = raw_doc.lower()\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","sentence_tockens = nltk.sent_tokenize(raw_doc)\n","word_tockens = nltk.word_tokenize(raw_doc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pEMhfG6SQbHM","executionInfo":{"status":"ok","timestamp":1706683565940,"user_tz":-330,"elapsed":1105,"user":{"displayName":"Akshit Sharma","userId":"06858188963884428867"}},"outputId":"b95e1916-bdc6-436d-a48c-133e7b4d0115"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}]},{"cell_type":"code","source":["sentence_tockens[2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"5uFe6oHTR6DD","executionInfo":{"status":"ok","timestamp":1706683796786,"user_tz":-330,"elapsed":4,"user":{"displayName":"Akshit Sharma","userId":"06858188963884428867"}},"outputId":"236f87dc-8d0e-4dcf-96cc-94653ef7cea7"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'the reason given is: this article is using citations from 1970 and virtually all claims about conversational capabilities are at least ten years out of date (for example the turing test was arguably made obsolete years ago by transformer models).'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["word_tockens[10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"LFtg8iLVSBUI","executionInfo":{"status":"ok","timestamp":1706683816921,"user_tz":-330,"elapsed":4,"user":{"displayName":"Akshit Sharma","userId":"06858188963884428867"}},"outputId":"8e00a07d-867f-4726-ba47-bfbad8c0073a"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'log'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["proforming text-preprocessing steps\n"],"metadata":{"id":"Q99Kqix6Slkw"}},{"cell_type":"code","source":["\n","lemmer = nltk.stem.WordNetLemmatizer()\n","\n","def LemTokens(tokens):\n","    return [lemmer.lemmatize(token) for token in tokens]\n","\n","remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n","\n","def LemNormalize(text):\n","    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n","\n"],"metadata":{"id":"cMQ_MfanSHHZ","executionInfo":{"status":"ok","timestamp":1706687094374,"user_tz":-330,"elapsed":397,"user":{"displayName":"Akshit Sharma","userId":"06858188963884428867"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["greeting functions"],"metadata":{"id":"zTfJuYGFTx3y"}},{"cell_type":"code","source":["greet_inputs = ('hello','hi','whassup','how are you?')\n","greet_response = ('hi','Hey','Hey There!')\n","def greet(sentence):\n","  for word in sentence.split():\n","    if word.lower() in greet_inputs:\n","      return random.choice(greet_response)"],"metadata":{"id":"2m3SbVbZTF1E","executionInfo":{"status":"ok","timestamp":1706687096001,"user_tz":-330,"elapsed":2,"user":{"displayName":"Akshit Sharma","userId":"06858188963884428867"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"hz0iw_oyU3UN","executionInfo":{"status":"ok","timestamp":1706687011227,"user_tz":-330,"elapsed":3,"user":{"displayName":"Akshit Sharma","userId":"06858188963884428867"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def response(user_response):\n","  robo1_response = ''\n","  TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english')\n","  tfidf = TfidfVec.fit_transform(sentence_tockens)\n","  vals =cosine_similarity(tfidf[-1],tfidf)\n","  idx = vals.argsort()[0][-2]\n","  flat = vals.flatten()\n","  flat.sort()\n","  req_tfidf = flat[-2]\n","  if(req_tfidf == 0):\n","    robo1_response = robo1_response + \"I am sorry.Unable to understand you!\"\n","    return robo1_response\n","  else:\n","    robo1_response = robo1_response+ sentence_tockens[idx]\n","    return robo1_response"],"metadata":{"id":"WfcmJJ9PWKDh","executionInfo":{"status":"ok","timestamp":1706687098110,"user_tz":-330,"elapsed":665,"user":{"displayName":"Akshit Sharma","userId":"06858188963884428867"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["flag = True\n","print('Hello! I am the Learning Bot. Start typing your text')\n","while(flag==True):\n","  user_response = input()\n","  user_response = user_response.lower()\n","  if(user_response !='bye'):\n","    if(user_response == 'thank you' or user_response == 'thanks'):\n","      flag=False\n","      print('Bot: you are welcome..')\n","    else:\n","        if(greet(user_response) !=None):\n","          print('Bot'+ greet(user_response))\n","        else:\n","          sentence_tockens.append(user_response)\n","          word_tockens = word_tockens + nltk.word_tokenize(user_response)\n","          final_words = list(set(word_tockens))\n","          print('Bot:' , end = '')\n","          print(response(user_response))\n","          sentence_tockens.remove(user_response)\n","  else:\n","    flag = False\n","    print('Bot: Goodbye!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wfk-lyaxYUG6","executionInfo":{"status":"ok","timestamp":1706687180454,"user_tz":-330,"elapsed":80386,"user":{"displayName":"Akshit Sharma","userId":"06858188963884428867"}},"outputId":"f9f2a2d2-e932-4d77-d407-ae7020e456c1"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello! I am the Learning Bot. Start typing your text\n","hii\n","Bot:"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["hii\n","what is chat bot\n","Bot:virtual agent chat.\n","application of chat bot]\n","Bot:virtual agent chat.\n","Application of chatbots\n","Bot:\"chatbots: history, technology, and applications\".\n","any other\n","Bot:I am sorry.Unable to understand you!\n","bye\n","Bot: Goodbye!\n"]}]}]}